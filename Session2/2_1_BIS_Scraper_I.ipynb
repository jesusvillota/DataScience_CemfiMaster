{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "771dbee0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jesusvillota/CSS_DataScience_2025/blob/main/Session2/2_1_BIS_Scraper_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c10e12",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 880px; margin: 20px auto 22px; padding: 0px; border-radius: 18px; border: 1px solid #e5e7eb; background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%); box-shadow: 0 8px 26px rgba(0,0,0,0.06); overflow: hidden;\">\n",
    "\n",
    "  <!-- Banner Header -->\n",
    "  <div style=\"padding: 34px 32px 14px; text-align: center; line-height: 1.38;\">\n",
    "    <div style=\"font-size: 13px; letter-spacing: 0.14em; text-transform: uppercase; color: #6b7280; font-weight: bold; margin-bottom: 5px;\">\n",
    "      Session #2\n",
    "    </div>\n",
    "    <div style=\"font-size: 29px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      Scraping Central Bank Speeches from the BIS\n",
    "    </div>\n",
    "    <div style=\"font-size: 29px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      Part I\n",
    "    </div>\n",
    "    <div style=\"font-size: 16.5px; color: #374151; font-style: italic; margin-bottom: 0;\">\n",
    "      Using Textual Data in Empirical Monetary Economics\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Logo Section -->\n",
    "  <div style=\"background: none; text-align: center; margin: 30px 0 10px;\">\n",
    "    <img src=\"https://www.cemfi.es/images/Logo-Azul.png\" alt=\"CEMFI Logo\" style=\"width: 158px; filter: drop-shadow(0 2px 12px rgba(56,84,156,0.05)); margin-bottom: 0;\">\n",
    "  </div>\n",
    "\n",
    "  <!-- Name -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1.22em; font-weight: bold; margin-bottom: 0px;\">\n",
    "    Jesus Villota Miranda Â© 2025\n",
    "  </div>\n",
    "\n",
    "  <!-- Contact info -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1em; margin-top: 7px; margin-bottom: 20px;\">\n",
    "    <a href=\"mailto:jesus.villota@cemfi.edu.es\" style=\"color: #38549c; text-decoration: none; margin-right:8px;\" title=\"Email\">\n",
    "      <!-- Email logo -->\n",
    "      <!-- <img src=\"https://cdn-icons-png.flaticon.com/512/11679/11679732.png\" alt=\"Email\" style=\"width:18px; vertical-align:middle; margin-right:5px;\"> -->\n",
    "      jesus.villota@cemfi.edu.es\n",
    "    </a>\n",
    "    <span style=\"color:#9fa7bd;\">|</span>\n",
    "    <a href=\"https://www.linkedin.com/in/jesusvillotamiranda/\" target=\"_blank\" style=\"color: #38549c; text-decoration: none; margin-left:7px;\" title=\"LinkedIn\">\n",
    "      <!-- LinkedIn logo -->\n",
    "      <!-- <img src=\"https://1.bp.blogspot.com/-onvhHUdW1Us/YI52e9j4eKI/AAAAAAAAE4c/6s9wzOpIDYcAo4YmTX1Qg51OlwMFmilFACLcBGAsYHQ/s1600/Logo%2BLinkedin.png\" alt=\"LinkedIn\" style=\"width:17px; vertical-align:middle; margin-right:5px;\"> -->\n",
    "      LinkedIn\n",
    "    </a>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574f945",
   "metadata": {},
   "source": [
    "**IMPORTANT**: **Are you running this notebook in Google Colab?**\n",
    "\n",
    "- If so, please make sure that in the cell below `running_in_colab` is set to `True`\n",
    "\n",
    "- And, of course,  make sure to **run the cell**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f1acb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARE YOU RUNNING THIS IN GOOGLE COLAB? If YES, type True below\n",
    "running_in_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0844aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conditional install ---\n",
    "if running_in_colab:\n",
    "    # Install selenium if running in Colab\n",
    "    !pip install bs4 requests selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec054f43",
   "metadata": {},
   "source": [
    "# **0. Introduction**\n",
    "\n",
    "In this notebook, we will explore how to automatically collect and process central bank speeches published by the Bank for International Settlements (BIS). We will demonstrate practical techniques for scraping web content, handling dynamic pages, and extracting information from documents.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.pngitem.com/pimgs/m/586-5865614_bis-bank-for-international-settlements-logo-hd-png.png\" alt=\"Bank for International Settlements\" width=\"250\"/>\n",
    "</div></div>\n",
    "\n",
    "## **Understanding the Website Structure**\n",
    "\n",
    "If you go to the website ([https://www.bis.org/cbspeeches](https://www.bis.org/cbspeeches)), you will see that there is a dynamic pane with the speeches, and you can only load up to a maximum of 25 speeches at a time. This means that we may need to implement pagination handling in our scraper to access all available speeches. If you were to run your scraping procedure on the simple link ([https://www.bis.org/cbspeeches](https://www.bis.org/cbspeeches), you would only be able to scrape the first 10 speeches (the default number of speeches loaded is 10), and nothing else! \n",
    "\n",
    "> **IMPORTANT**: A crucial step when scraping this type of websites is to tweak the parameters of the selection menu and look at the link. Then, you can infer the patterns to dynamically scrape the content from all the pages. \n",
    "\n",
    "Try it out yourself! Go to the BIS webpage, and make adjustments in the menu. Then look again at the link. You will see that the link now is different!\n",
    "\n",
    "**Modus Operandi**\n",
    "\n",
    "1) Go to the link: From the original link: ([https://www.bis.org/cbspeeches](https://www.bis.org/cbspeeches))\n",
    "2) Make modifications in the selection menu\n",
    "\n",
    "> ![](images/selection1.png)\n",
    "\n",
    "3) Scroll down to modify the amount of speeches shown per page\n",
    "\n",
    "> ![](images/selection2.png)\n",
    "\n",
    "If you now go to the link, you will see it changed!\n",
    "\n",
    "> ![](images/link.png)\n",
    "> https://www.bis.org/cbspeeches?fromDate=01%2F01%2F2008&tillDate=19%2F08%2F2025&authors=2366&authors=2864&institutions=1&institutions=29&countries=19&countries=168&cbspeeches_page=3&cbspeeches_page_length=25\n",
    "\n",
    "\n",
    "Let's break down this link:\n",
    "\n",
    "\n",
    "> `https://www.bis.org/cbspeeches`\n",
    ">\n",
    "> ?`fromDate`=**01**%2F**01**%2F**2008**\n",
    "> \n",
    "> &`tillDate`=**19**%2F**08**%2F**2025**\n",
    ">\n",
    "> &`authors`=**2366**&`authors`=**2864**\n",
    ">\n",
    "> &`institutions`=**1**&`institutions`=**29**\n",
    ">\n",
    "> &`countries`=**19**&`countries`=**168**\n",
    ">\n",
    "> &`cbspeeches_page`=**3**\n",
    ">\n",
    "> &`cbspeeches_page_length`=**25**\n",
    "\n",
    "\n",
    "As you can see, whatever you do in the selection pane is mapped into the link! This provides us a way to easily scrape with little effort by simply modifying the URL parameters.\n",
    "\n",
    "For this purpose, let's define a function that will take us from a set of arguments (`initial_date`, `final_date`, `page`, `page_length`) into the desired link to be scraped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b272b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bis_link(initial_date, final_date, page, page_length): \n",
    "    index_url = (\n",
    "        f\"https://www.bis.org/cbspeeches\"\n",
    "        f\"?fromDate={initial_date}\"\n",
    "        f\"&tillDate={final_date}\"\n",
    "        f\"&cbspeeches_page={page}\"\n",
    "        f\"&cbspeeches_page_length={page_length}\"\n",
    "    )\n",
    "    return index_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6afaeb",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080ed445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.bis.org/cbspeeches?fromDate=01/01/2010&tillDate=01/01/2020&cbspeeches_page=2&cbspeeches_page_length=10'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bis_link(\n",
    "    \"01/01/2010\",\n",
    "    \"01/01/2020\",\n",
    "    2,\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27529b8c",
   "metadata": {},
   "source": [
    "Now, let's define the parameters as global variables to be used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5088362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- Setup params ---\n",
    "BASE_URL = \"https://www.bis.org\"\n",
    "INITIAL_DATE = \"01/01/2000\"\n",
    "FINAL_DATE = \"11/08/2025\"\n",
    "PAGE_LENGTH = 25\n",
    "MAX_PAGE = 1\n",
    "DOWNLOAD_DIR = Path(\"output/2_2/comparison\")\n",
    "\n",
    "if running_in_colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  DOWNLOAD_DIR = Path(\"/content/gdrive/My Drive\") / DOWNLOAD_DIR\n",
    "else: \n",
    "  DOWNLOAD_DIR = Path(\"../\") / DOWNLOAD_DIR\n",
    "\n",
    "# Create directories using pathlib\n",
    "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159143a",
   "metadata": {},
   "source": [
    "# **1. Scraping from the raw HTML (*Naive Approach*)**\n",
    "\n",
    "- Let's try to apply what we learned on Monday in Session#1. \n",
    "- If you remember, we learned how to scrape a static webpage ([www.cemfi.es](https://www.cemfi.es)) using `requests` and `BeautifulSoup`. \n",
    "- Now, we'll attempt to scrape the BIS website of central bank speeches ([https://www.bis.org/cbspeeches](https://www.bis.org/cbspeeches)) using the same approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a560f30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing page 1 with naive approach ===\n",
      "ð Requesting URL: https://www.bis.org/cbspeeches?fromDate=01/01/2000&tillDate=11/08/2025&cbspeeches_page=1&cbspeeches_page_length=25\n",
      "ð HTTP Status: 200\n",
      "â Could not find container with id 'cbspeeches_list'\n",
      "ð¾ Saved HTML to ../output/2_2/comparison/page_1_naive.html for inspection\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Let's try the naive approach using just requests and BeautifulSoup\n",
    "for i in range(1, MAX_PAGE+1):\n",
    "    index_url = bis_link(INITIAL_DATE, FINAL_DATE, i, PAGE_LENGTH)\n",
    "    \n",
    "    print(f\"\\n=== Processing page {i} with naive approach ===\")\n",
    "    print(f\"ð Requesting URL: {index_url}\")\n",
    "    \n",
    "    # Step 1. Send a simple HTTP request - (this won't execute any JavaScript)\n",
    "    response = requests.get(index_url)\n",
    "    print(f\"ð HTTP Status: {response.status_code}\")\n",
    "    \n",
    "    # Step 2. Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Step 3. Try to find the speeches container and links\n",
    "    container = soup.find(id=\"cbspeeches_list\")\n",
    "    \n",
    "    if container:\n",
    "        print(\"â Found container with id 'cbspeeches_list'!\")\n",
    "        # Try to find speech links\n",
    "        review_links = container.select(\"a.dark[href^='/review/']\")\n",
    "        print(f\"â Found {len(review_links)} review links on page {i}.\")\n",
    "        \n",
    "        # Print the first few links to see what we got\n",
    "        for link in review_links[:3]:\n",
    "            print(f\"Link: {link.get('href')}\")\n",
    "    else:\n",
    "        print(\"â Could not find container with id 'cbspeeches_list'\")\n",
    "    \n",
    "    # Save the HTML\n",
    "    output_file = DOWNLOAD_DIR / f\"page_{i}_naive.html\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "    print(f\"ð¾ Saved HTML to {output_file} for inspection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d086590",
   "metadata": {},
   "source": [
    "# **2. Overcoming the Challenge of Scraping JavaScript-heavy websites:**  \n",
    "\n",
    "1. **The Problem**: When using simple HTTP requests (via `requests` or similar libraries), we only get the initial HTML from the server, which often lacks the content we see in the browser.\n",
    "\n",
    "2. **Why This Happens**: Modern websites load content dynamically after the initial page load using JavaScript. The browser executes this JavaScript to fetch and display data, but basic HTTP requests don't.\n",
    "\n",
    "3. **Solutions**:\n",
    "   - Analyzing potential API endpoints that the website might be using\n",
    "   - Using `Selenium` to automate a real browser that executes JavaScript\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Always check if a website uses JavaScript to load content before choosing a scraping approach\n",
    "   - Use browser dev tools (especially the Network tab) to understand how data is loaded\n",
    "   - Consider browser automation for complex sites or direct API requests for efficiency\n",
    "   - Be respectful of websites' terms of service and implement rate limiting\n",
    "\n",
    "Conclusion: The approach you choose depends on the specific website, your performance requirements, and the complexity of the data you need to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3973dac",
   "metadata": {},
   "source": [
    "## **2.1. Looking for an API endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542694f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to access potential API endpoint: https://www.bis.org/cbspeeches?fromDate=01/01/2000&tillDate=11/08/2025&cbspeeches_page=1&cbspeeches_page_length=25\n",
      "ð API Response Status: 200\n",
      "Response is not JSON. Content type: text/html; charset=UTF-8\n",
      "\n",
      "First 500 characters of response: <!DOCTYPE html>\n",
      "<html class='no-js' lang='en' xml:lang='en' xmlns='http://www.w3.org/1999/xhtml'>\n",
      "<head>\n",
      "<meta content='IE=edge' http-equiv='X-UA-Compatible'>\n",
      "<meta content='width=device-width, initial-scale=1.0' name='viewport'>\n",
      "<meta content='text/html; charset=utf-8' http-equiv='Content-Type'>\n",
      "<meta content='Central bankers&#39; speeches' property='og:title'>\n",
      "<meta content='Central bankers&#39; speeches' property='og:description'>\n",
      "<meta content='https://www.bis.org/cbspeeches/index.htm' prope\n",
      "\n",
      "Note: Finding the exact API endpoint requires analyzing network traffic in browser dev tools.\n",
      "The URL used above is just a guess for demonstration purposes!\n"
     ]
    }
   ],
   "source": [
    "# Let's try to find and use the API endpoint that the BIS website might be using\n",
    "# This is a common approach for dealing with JavaScript-heavy sites\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Based on network analysis (done in browser), we might find that the site uses an API endpoint\n",
    "# For demonstration purposes, let's try a common pattern for such endpoints\n",
    "\n",
    "# Use bis_link function to generate the URL instead of hardcoding\n",
    "api_url = bis_link(INITIAL_DATE, FINAL_DATE, 1, PAGE_LENGTH)\n",
    "\n",
    "print(f\"Attempting to access potential API endpoint: {api_url}\")\n",
    "\n",
    "try:\n",
    "    # Some APIs require headers that look like a browser to prevent scraping\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Referer\": \"https://www.bis.org/cbspeeches/index.htm\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    print(f\"ð API Response Status: {response.status_code}\")\n",
    "    \n",
    "    # Try to parse as JSON\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            print(\"â Successfully parsed JSON response!\")\n",
    "            print(f\"Response structure: {json.dumps(data, indent=2)[:500]}...\")  # Print first 500 chars\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Response is not JSON. Content type:\", response.headers.get('Content-Type'))\n",
    "            print(\"\\nFirst 500 characters of response:\", response.text[:500])\n",
    "    else:\n",
    "        print(f\"â Failed to access API endpoint: {response.status_code}\")\n",
    "        print(\"\\nFirst 500 characters of response:\", response.text[:500])\n",
    "        \n",
    "    # Note to students: If the above doesn't work, you would need to:\n",
    "    # 1. Use browser dev tools (Network tab) to see what requests are being made when the page loads\n",
    "    # 2. Look for XHR or Fetch requests that return the data you need\n",
    "    # 3. Analyze those requests and replicate them in your code\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"â Error occurred: {e}\")\n",
    "\n",
    "print(\"\\nNote: Finding the exact API endpoint requires analyzing network traffic in browser dev tools.\")\n",
    "print(\"The URL used above is just a guess for demonstration purposes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a31017d",
   "metadata": {},
   "source": [
    "## **2.2. Selenium approach: JavaScript Rendering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010fda9b",
   "metadata": {},
   "source": [
    "As we just saw, the naive approach using `requests` and `BeautifulSoup` failed to capture the content we need. This is because the BIS website uses JavaScript to dynamically load the speeches data after the initial page load.\n",
    "\n",
    "To overcome this limitation, we'll use **Selenium** - a browser automation tool that:\n",
    "\n",
    "1. Opens a real browser instance\n",
    "2. Navigates to the BIS website \n",
    "3. Allows JavaScript to fully execute and render the page\n",
    "4. Gives us access to the complete HTML content that users actually see\n",
    "\n",
    "This approach simulates how a human would interact with the website, ensuring we can access all the dynamically loaded content. In the following sections, we'll:\n",
    "\n",
    "- Set up Selenium with Chrome WebDriver\n",
    "- Visit the BIS speeches page\n",
    "- Wait for the JavaScript to execute and render the content\n",
    "- Extract the links to individual speeches\n",
    "- Compare the results with our naive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d295e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing page 1 with Selenium ===\n",
      "ð¾ Saved JavaScript-rendered HTML to ../output/2_2/comparison/page_1_selenium.html\n",
      "â Found 25 review links with Selenium.\n",
      "Link: https://www.bis.org/review/r250728g.htm\n",
      "Link: https://www.bis.org/review/r250728f.htm\n",
      "Link: https://www.bis.org/review/r250728e.htm\n",
      "Link: https://www.bis.org/review/r250717g.htm\n",
      "Link: https://www.bis.org/review/r250728i.htm\n",
      "Link: https://www.bis.org/review/r250728h.htm\n",
      "Link: https://www.bis.org/review/r250717f.htm\n",
      "Link: https://www.bis.org/review/r250728d.htm\n",
      "Link: https://www.bis.org/review/r250717b.htm\n",
      "Link: https://www.bis.org/review/r250728j.htm\n",
      "Link: https://www.bis.org/review/r250728k.htm\n",
      "Link: https://www.bis.org/review/r250714a.htm\n",
      "Link: https://www.bis.org/review/r250717h.htm\n",
      "Link: https://www.bis.org/review/r250728l.htm\n",
      "Link: https://www.bis.org/review/r250709f.htm\n",
      "Link: https://www.bis.org/review/r250716a.htm\n",
      "Link: https://www.bis.org/review/r250717e.htm\n",
      "Link: https://www.bis.org/review/r250701c.htm\n",
      "Link: https://www.bis.org/review/r250715c.htm\n",
      "Link: https://www.bis.org/review/r250715b.htm\n",
      "Link: https://www.bis.org/review/r250715a.htm\n",
      "Link: https://www.bis.org/review/r250717a.htm\n",
      "Link: https://www.bis.org/review/r250715d.htm\n",
      "Link: https://www.bis.org/review/r250717c.htm\n",
      "Link: https://www.bis.org/review/r250728m.htm\n"
     ]
    }
   ],
   "source": [
    "# Let's use Selenium to get the \"real\" HTML after JavaScript execution\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "if running_in_colab:\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless') # Run in headless mode\n",
    "    chrome_options.add_argument('--no-sandbox') # Bypass OS security model\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage') # Overcome limited resource problems\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "else: \n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Use the bis_link function instead of hardcoding the URL\n",
    "    index_url = bis_link(INITIAL_DATE, FINAL_DATE, 1, PAGE_LENGTH)\n",
    "    print(f\"\\n=== Processing page {i} with Selenium ===\")\n",
    "    \n",
    "    # Navigate to the page and wait for JavaScript to execute\n",
    "    driver.get(index_url)\n",
    "    time.sleep(5)  # Wait for JS to load content\n",
    "    \n",
    "    # Save the HTML after JavaScript execution\n",
    "    selenium_html = driver.page_source\n",
    "    output_file = DOWNLOAD_DIR / \"page_1_selenium.html\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(selenium_html)\n",
    "    print(f\"ð¾ Saved JavaScript-rendered HTML to {output_file}\")\n",
    "    \n",
    "    # Try to find the speeches container and links\n",
    "    try:\n",
    "        container = driver.find_element(By.ID, \"cbspeeches_list\")\n",
    "        review_links = container.find_elements(By.CSS_SELECTOR, \"a.dark[href^='/review/']\")\n",
    "        review_hrefs = [link.get_attribute(\"href\") for link in review_links]\n",
    "        print(f\"â Found {len(review_hrefs)} review links with Selenium.\")\n",
    "        \n",
    "        # Print the first few links\n",
    "        for href in review_hrefs:\n",
    "            print(f\"Link: {href}\")\n",
    "    except Exception as e:\n",
    "        print(f\"â Could not find review links with Selenium: {e}\")\n",
    "        \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e55b104",
   "metadata": {},
   "source": [
    "# **3. Comparison of the HTML contents: *Naive vs Selenium***\n",
    "\n",
    "The naive approach only captures the initial HTML, which lacks the dynamically loaded content. In contrast, the Selenium approach provides the fully rendered HTML, including all JavaScript-generated elements.\n",
    "\n",
    "This comparison highlights the importance of using the right tools for web scraping, especially when dealing with modern websites that rely heavily on JavaScript for content delivery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e14c950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Size comparison ---\n",
      "Naive HTML size: 20,924 bytes\n",
      "Selenium HTML size: 114,818 bytes\n",
      "Difference: 93,894 bytes (448.7% more content in Selenium version)\n",
      "\n",
      "--- Content Analysis ---\n",
      "â Naive HTML does NOT have the speeches container\n",
      "â Selenium HTML has the speeches container\n",
      "\n",
      "--- DOM Structure Differences ---\n",
      "IDs present in Selenium HTML but not in naive HTML: ['nav_main_menu', 'main_menu', 'dtmenu', 'menuline', 'toptitle', 'dthome', 'breadcrumbs', 'nav_local_menu', 'local_menu', 'cbspeeches']\n"
     ]
    }
   ],
   "source": [
    "# Let's analyze the differences between the two HTML files in more detail\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "# Only run this if both files exist\n",
    "naive_file = DOWNLOAD_DIR / \"page_1_naive.html\"\n",
    "selenium_file = DOWNLOAD_DIR / \"page_1_selenium.html\"\n",
    "\n",
    "if naive_file.exists() and selenium_file.exists():\n",
    "    # Read both HTML files\n",
    "    with open(naive_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        naive_html = f.read()\n",
    "    \n",
    "    with open(selenium_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        selenium_html = f.read()\n",
    "    \n",
    "    # Parse with BeautifulSoup for better analysis\n",
    "    naive_soup = BeautifulSoup(naive_html, 'html.parser')\n",
    "    selenium_soup = BeautifulSoup(selenium_html, 'html.parser')\n",
    "\n",
    "    print(\"--- Size comparison ---\")\n",
    "\n",
    "    # Compare file sizes\n",
    "    naive_size = len(naive_html)\n",
    "    selenium_size = len(selenium_html)\n",
    "    size_diff = selenium_size - naive_size\n",
    "    print(f\"Naive HTML size: {naive_size:,} bytes\")\n",
    "    print(f\"Selenium HTML size: {selenium_size:,} bytes\")\n",
    "    print(f\"Difference: {size_diff:,} bytes ({size_diff/naive_size*100:.1f}% more content in Selenium version)\")\n",
    "    \n",
    "    # Check for specific content we're interested in\n",
    "    naive_speeches = naive_soup.find(id=\"cbspeeches_list\")\n",
    "    selenium_speeches = selenium_soup.find(id=\"cbspeeches_list\")\n",
    "    \n",
    "    print(\"\\n--- Content Analysis ---\")\n",
    "    if naive_speeches:\n",
    "        print(f\"â Naive HTML has the speeches container\")\n",
    "    else:\n",
    "        print(\"â Naive HTML does NOT have the speeches container\")\n",
    "    \n",
    "    if selenium_speeches:\n",
    "        print(f\"â Selenium HTML has the speeches container\")\n",
    "    else:\n",
    "        print(\"â Selenium HTML does NOT have the speeches container\")\n",
    "\n",
    "    # Find DOM differences that might explain where the dynamic content goes\n",
    "    print(\"\\n--- DOM Structure Differences ---\")\n",
    "    \n",
    "    # Look for elements that exist in selenium but not in naive HTML\n",
    "    selenium_ids = [el.get('id') for el in selenium_soup.find_all(id=True)]\n",
    "    naive_ids = [el.get('id') for el in naive_soup.find_all(id=True)]\n",
    "    \n",
    "    added_ids = [id for id in selenium_ids if id not in naive_ids]\n",
    "    print(f\"IDs present in Selenium HTML but not in naive HTML: {added_ids[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "css-datascience-2025-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
